{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a5d6f9-979a-4b18-9222-ae6ee83c386d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q 1 Answer:\n",
    "\"\"\"\n",
    "Missing values in a dataset refer to the absence of a particular value or information for a specific attribute or feature of a data point. \n",
    "These values can be represented by null, NaN, or any other placeholder value.\n",
    "\n",
    "It is essential to handle missing values in a dataset because they can cause various problems, \n",
    "including biased or incomplete analysis, incorrect statistical measures, \n",
    "and reduced accuracy of machine learning models. \n",
    "Ignoring missing values or deleting the entire data point containing them can lead to a loss of information and affect the quality of the \n",
    "analysis or model.\n",
    "\n",
    "Some algorithms that are not affected by missing values include:\n",
    "\n",
    "1. Decision Trees: Decision trees can handle missing values by placing them in a separate branch of the tree, \n",
    "thus preventing them from affecting the decision process.\n",
    "\n",
    "2. Random Forest: Random Forest is an ensemble learning algorithm that uses multiple decision trees. \n",
    "It can handle missing values by imputing them using the median value of the feature in the training data.\n",
    "\n",
    "3.K-Nearest Neighbors (KNN): KNN is a non-parametric algorithm that uses the nearest neighbors to classify or predict the target variable.\n",
    "It can handle missing values by imputing them using the mean or mode value of the feature in the training data.\n",
    "\n",
    "4.Support Vector Machines (SVM): SVM is a supervised learning algorithm that finds the optimal hyperplane to separate the data into different classes. \n",
    "It can handle missing values by ignoring them during the optimization process.\n",
    "\n",
    "5.Naive Bayes: Naive Bayes is a probabilistic algorithm that uses Bayes' theorem to calculate the probability of a particular class. \n",
    "It can handle missing values by ignoring them during the probability calculation.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e6a458-2b34-4e3e-84ef-32a115d722af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q 2 Answer:\n",
    "\"\"\"\n",
    "There are several techniques to handle missing data in a dataset. Here are some of the most common techniques and examples of \n",
    "how to implement them in Python:\n",
    "\n",
    "1. Deleting Rows with Missing Values\n",
    "This technique involves removing the entire row from the dataset that contains missing values. \n",
    "This method is effective when the amount of missing data is minimal.\n",
    "Example Code:\n",
    "\n",
    "     import pandas as pd\n",
    "\n",
    "     # read the dataset\n",
    "     data = pd.read_csv('dataset.csv')\n",
    "\n",
    "    # drop rows with missing values\n",
    "    data.dropna(inplace=True)\n",
    "\n",
    "    # print the cleaned dataset\n",
    "    print(data)\n",
    "\n",
    "2. Deleting Columns with Missing Values\n",
    "This technique involves removing the entire column from the dataset that contains missing values. \n",
    "This method is effective when the feature is not essential for the analysis or modeling.\n",
    "Example Code:\n",
    "            import pandas as pd\n",
    "\n",
    "            # read the dataset\n",
    "            data = pd.read_csv('dataset.csv')\n",
    "\n",
    "            # drop columns with missing values\n",
    "            data.dropna(axis=1, inplace=True)\n",
    "\n",
    "            # print the cleaned dataset\n",
    "            print(data)\n",
    "\n",
    "\n",
    "3. Imputing Missing Values with Mean, Median or Mode\n",
    "This technique involves replacing missing values with the mean, median or mode of the feature in the dataset.\n",
    "Example Code:\n",
    "             import pandas as pd\n",
    "\n",
    "                # read the dataset\n",
    "                data = pd.read_csv('dataset.csv')\n",
    "\n",
    "                # impute missing values with mean\n",
    "                data.fillna(data.mean(), inplace=True)\n",
    "\n",
    "                # print the cleaned dataset\n",
    "                print(data)\n",
    "\n",
    "4. Imputing Missing Values with Regression\n",
    "This technique involves using a regression model to impute missing values by predicting the missing values based on the relationship with other\n",
    "variables in the dataset.\n",
    "Example Code: \n",
    "                import pandas as pd\n",
    "                from sklearn.linear_model import LinearRegression\n",
    "\n",
    "                # read the dataset\n",
    "                data = pd.read_csv('dataset.csv')\n",
    "\n",
    "                # create a linear regression model\n",
    "                regressor = LinearRegression()\n",
    "\n",
    "                # split the dataset into two parts, one with missing values and one without missing values\n",
    "                data_missing = data[data.isnull().any(axis=1)]\n",
    "                data_not_missing = data.dropna()\n",
    "\n",
    "                # fit the model on the data without missing values\n",
    "                regressor.fit(data_not_missing.drop('target', axis=1), data_not_missing['target'])\n",
    "\n",
    "                # predict the missing values\n",
    "                imputed_values = regressor.predict(data_missing.drop('target', axis=1))\n",
    "\n",
    "                # fill the missing values with the predicted values\n",
    "                data.loc[data.isnull().any(axis=1), 'target'] = imputed_values\n",
    "\n",
    "                # print the cleaned dataset\n",
    "                print(data)\n",
    "\n",
    "5. Imputing Missing Values with K-Nearest Neighbors\n",
    "This technique involves imputing missing values with the mean or median of the k-nearest neighbors in the dataset.\n",
    "Example Code:\n",
    "            import pandas as pd\n",
    "            from sklearn.impute import KNNImputer\n",
    "\n",
    "            # read the dataset\n",
    "            data = pd.read_csv('dataset.csv')\n",
    "\n",
    "            # create a KNN imputer object\n",
    "            imputer = KNNImputer(n_neighbors=3)\n",
    "\n",
    "            # impute the missing values\n",
    "            imputed_data = imputer.fit_transform(data)\n",
    "\n",
    "            # convert the imputed data to a dataframe\n",
    "            data_imputed = pd.DataFrame(imputed_data, columns=data.columns)\n",
    "\n",
    "            # print the cleaned dataset\n",
    "            print(data_imputed)\n",
    "\n",
    "          \n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7570b622-897a-4528-bd45-d5ebdf5f0fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q 3 Answer:\n",
    "\"\"\"\n",
    "mbalanced data is a situation in which the classes or categories of a target variable in a dataset are not represented equally. \n",
    "In other words, there is a significant difference in the number of instances between the different classes, \n",
    "with one class having a much smaller number of instances compared to the other class(es). \n",
    "For example, in a binary classification problem, if 90% of the instances belong to class A, and only 10% belong to class B, \n",
    "the dataset is said to be imbalanced.\n",
    "\n",
    "If imbalanced data is not handled properly, it can lead to biased models that perform poorly in predicting the minority class. \n",
    "In such a scenario, the model will be biased towards the majority class, as it will be more prominent in the dataset,\n",
    "resulting in poor predictive performance for the minority class. In addition, \n",
    "accuracy is not a reliable metric for measuring the performance of a model on an imbalanced dataset, \n",
    "as the model can achieve high accuracy by simply predicting the majority class.\n",
    "\n",
    "Some of the problems that can arise if imbalanced data is not handled include:\n",
    "\n",
    "1. Poor predictive performance for the minority class\n",
    "2. Biased models that prioritize the majority class\n",
    "3. Overfitting of the model due to the imbalance in the dataset\n",
    "4. Difficulty in detecting the minority class during testing\n",
    "\n",
    "\n",
    "Therefore, it is essential to handle imbalanced data in a dataset to ensure that the model is accurate in predicting all classes, \n",
    "and not just the majority class. Some of the techniques used to handle imbalanced data include oversampling the minority class,\n",
    "undersampling the majority class, using cost-sensitive learning algorithms, and using ensemble methods such as bagging, boosting, and stacking.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbb42dd-7c7c-440d-b9ce-8cccf138ccc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q 4 Answer:\n",
    "\"\"\"\n",
    "Up-sampling and down-sampling are two common techniques used to handle imbalanced data in a dataset. These techniques involve manipulating the dataset to balance the classes of the \n",
    "target variable by either increasing or decreasing the number of instances in each class.\n",
    "\n",
    "Down-sampling involves reducing the number of instances in the majority class, \n",
    "while up-sampling involves increasing the number of instances in the minority class. \n",
    "Here is an example to illustrate when up-sampling and down-sampling are required:\n",
    "\n",
    "Suppose we have a dataset of customer reviews for a product, and we want to build a sentiment analysis model to predict whether a \n",
    "review is positive or negative. In the dataset, we have 1000 reviews, out of which 800 are positive, and only 200 are negative.\n",
    "In this case, we have an imbalanced dataset because the positive class is overrepresented, and the negative class is underrepresented.\n",
    "\n",
    "Down-sampling: If we choose to use down-sampling to handle the imbalanced data, we would randomly select 200 instances \n",
    "from the majority class (positive) to match the number of instances in the minority class (negative). This will result\n",
    "in a balanced dataset with an equal number of positive and negative instances.\n",
    "\n",
    "Up-sampling: If we choose to use up-sampling to handle the imbalanced data, we would duplicate the instances in the minority class (negative)\n",
    "to match the number of instances in the majority class (positive). This will result in a balanced dataset with an equal number of \n",
    "positive and negative instances.\n",
    "\n",
    "When to use Up-sampling and Down-sampling: The choice of whether to use up-sampling or down-sampling depends on the specific problem and dataset. \n",
    "Down-sampling is typically used when the majority class has a much larger number of instances compared to the minority class, \n",
    "and the available data is sufficient to represent the majority class even after the downsampling. \n",
    "Up-sampling is typically used when the minority class has a small number of instances and more data is needed to train the model effectively.\n",
    "\n",
    "In summary, up-sampling and down-sampling are techniques used to handle imbalanced data in a dataset. \n",
    "These techniques involve manipulating the dataset to balance the classes of the target variable. \n",
    "The choice of which technique to use depends on the specific problem and dataset.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d7aca2-feef-4655-a5b8-e768fd8596e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q 5 Answer:\n",
    "\n",
    "\"\"\"\"\n",
    "Data augmentation is a technique used to artificially increase the size of a dataset by generating new examples from the existing ones. \n",
    "The idea behind data augmentation is to create new data points by applying various transformations to the existing ones,\n",
    "such as rotating, flipping, zooming, cropping, or adding noise, to name a few. \n",
    "This helps to increase the diversity of the data and make the model more robust to variations in the input data.\n",
    "\n",
    "SMOTE (Synthetic Minority Over-sampling Technique) is a popular data augmentation technique used to handle imbalanced data in a dataset. \n",
    "The technique involves generating synthetic samples of the minority class by interpolating between the existing minority class samples.\n",
    "SMOTE selects a random minority class instance and calculates the k-nearest neighbors (k-NN) of this instance. \n",
    "The synthetic samples are then generated by randomly selecting one of the k-NN instances and creating a new sample that is a combination of \n",
    "the original instance and the selected neighbor.\n",
    "\n",
    "The SMOTE algorithm can be summarized in the following steps:\n",
    "\n",
    "1. For each minority class instance, find its k-nearest neighbors.\n",
    "2. Randomly select one of the k-nearest neighbors.\n",
    "3. Generate a new synthetic sample by interpolating between the selected neighbor and the original instance.\n",
    "4. Repeat steps 2-3 until the desired number of synthetic samples is generated.\n",
    "\n",
    "\n",
    "SMOTE has been shown to be effective in balancing the classes of imbalanced datasets and improving the predictive performance of models trained \n",
    "on such datasets. However, it should be used with caution, as it can sometimes generate synthetic samples that are too similar to the original ones,\n",
    "leading to overfitting of the model.\n",
    "\n",
    "In summary, data augmentation is a technique used to artificially increase the size of a dataset by generating new examples from the existing ones. \n",
    "SMOTE is a popular data augmentation technique used to handle imbalanced data in a dataset, \n",
    "which involves generating synthetic samples of the minority class by interpolating between the existing minority class samples.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29c2c92-5cd1-4c7f-abdc-6bd4f06c593b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q 6 Answer:\n",
    "\"\"\"\n",
    "Outliers are data points in a dataset that significantly deviate from the other data points. \n",
    "They are extreme values that are much larger or much smaller than the other values in the dataset.\n",
    "Outliers can be caused by errors in data collection or measurement, or they can represent true anomalies or rare events in the data.\n",
    "\n",
    "It is essential to handle outliers because they can have a significant impact on the results of data analysis and machine learning models. \n",
    "Outliers can distort the statistical properties of a dataset, such as the mean and standard deviation, \n",
    "leading to incorrect interpretations and conclusions. In machine learning,\n",
    "outliers can have a significant impact on the performance of the model by influencing the estimated coefficients or weights used in the model.\n",
    "\n",
    "Handling outliers is important to ensure that the data analysis and machine learning models are based on accurate and reliable data.\n",
    "There are several techniques that can be used to handle outliers, including:\n",
    "\n",
    "Removal: One way to handle outliers is to remove them from the dataset. \n",
    "This approach should be used with caution because it can lead to a loss of information and bias in the remaining data.\n",
    "\n",
    "Transformation: Another approach is to transform the data using techniques such as log transformation, square root transformation,\n",
    "or Box-Cox transformation. These transformations can help to normalize the distribution of the data and reduce the impact of outliers.\n",
    "\n",
    "Winsorization: Winsorization is a method that replaces extreme values with less extreme values. \n",
    "The most extreme values are replaced with the next highest or lowest value in the dataset.\n",
    "\n",
    "Modeling: Another approach is to use robust statistical models that are less sensitive to outliers, such as the median or trimmed mean.\n",
    "In machine learning, robust models such as decision trees and random forests are less sensitive to outliers compared to linear regression and \n",
    "neural networks.\n",
    "\n",
    "In summary, outliers are extreme values in a dataset that can significantly affect data analysis and machine learning models. \n",
    "Handling outliers is essential to ensure that the data analysis and models are based on accurate and reliable data. \n",
    "There are several techniques available to handle outliers, and the choice of technique depends on the specific problem and dataset.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787cbfaf-3594-47f2-9e61-40d138ec7f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q 7 Answer:\n",
    "\"\"\"\n",
    "There are several techniques that can be used to handle missing data in a dataset. \n",
    "The choice of technique depends on the specific problem and dataset. Some common techniques for handling missing data are:\n",
    "\n",
    "1. Deletion: One way to handle missing data is to delete the rows or columns that contain missing values. \n",
    "This approach is straightforward but can lead to a loss of information and bias in the remaining data.\n",
    "\n",
    "2. Imputation: Imputation is the process of replacing missing values with estimated values based on the available data. \n",
    "This approach can be done using different methods, such as mean imputation, median imputation, mode imputation, or regression imputation.\n",
    "\n",
    "3. Hot-deck imputation: Hot-deck imputation is a method that replaces missing values with a randomly selected value from a similar record. \n",
    "The similar record is identified based on similarity in other variables in the dataset.\n",
    "\n",
    "4. Multiple imputation: Multiple imputation is a method that creates multiple imputed datasets with different plausible values for the missing values.\n",
    "The analyses are then performed on each imputed dataset, and the results are combined to provide an overall estimate.\n",
    "\n",
    "5. Machine learning-based imputation: Machine learning models can be trained to predict missing values based on the available data. \n",
    "For instance, decision trees or regression models can be used to impute missing values.\n",
    "\n",
    "In the case of analyzing customer data with missing values, one or more of the above techniques can be applied to handle the missing data.\n",
    "For example, if the percentage of missing data is relatively small, the data rows with missing values can be removed. \n",
    "However, if the percentage of missing data is significant, imputation techniques such as mean imputation,\n",
    "hot-deck imputation, or machine learning-based imputation can be applied. The specific technique used will depend on the type of missing data,\n",
    "the nature of the variables, and the goals of the analysis.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3a1d6b-ed74-4630-ae62-2dd1e95b5faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q 8 Answer:\n",
    "\"\"\"\n",
    "When dealing with missing data in a large dataset, it is essential to determine if the missing data is missing at random (MAR)\n",
    "or if there is a pattern to the missing data, known as non-random missing data (NMAR). \n",
    "MAR implies that the probability of a data point being missing is not related to the missing value\n",
    "itself but is only related to other observed data points. \n",
    "In contrast, NMAR means that the probability of a data point being missing is related to the missing value itself.\n",
    "\n",
    "There are several strategies that can be used to determine if the missing data is MAR or NMAR, including:\n",
    "\n",
    "1. Visual inspection: Visual inspection of the data can help identify patterns in the missing data. For instance, \n",
    "   plotting missing data against a particular variable or combination of variables can help identify any systematic patterns in the missing data.\n",
    "\n",
    "2. Statistical tests: There are various statistical tests that can be performed to test for the MAR assumption.\n",
    "   One such test is the Little's MCAR test, which tests whether the missing data is completely random or not.\n",
    "\n",
    "3. Correlation analysis: Correlation analysis can be performed to check if there is any correlation between the missing values and other variables.\n",
    "   If there is a correlation between the missing values and other variables, it suggests that the data is NMAR.\n",
    "\n",
    "4. Machine learning-based imputation: Machine learning models can be trained to predict missing values based on the available data. \n",
    "  The performance of the machine learning model can help identify if the missing data is NMAR or MAR.\n",
    "\n",
    "5. Sensitivity analysis: Sensitivity analysis involves testing different scenarios to determine how sensitive the results are to different assumptions. \n",
    "   For example, different imputation techniques can be tested to determine how sensitive the results are to different assumptions about the nature\n",
    "   of the missing data.\n",
    "\n",
    "\n",
    "\n",
    "In summary, there are several strategies that can be used to determine if the missing data is MAR or NMAR, including visual inspection,\n",
    "statistical tests, correlation analysis, machine learning-based imputation, and sensitivity analysis. These strategies can help ensure that \n",
    "the assumptions made about the nature of the missing data are correct and can inform the choice of the appropriate imputation method.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacfedcd-c22b-4ebd-88f1-6af26eaa44b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q 9 Answer:\n",
    "\"\"\"\n",
    "When dealing with an imbalanced dataset, such as in the case of a medical diagnosis project where the majority of patients do not have the condition \n",
    "of interest,\n",
    "it is essential to use appropriate evaluation strategies to ensure that the machine learning model's performance is accurately assessed.\n",
    "\n",
    "Some strategies that can be used to evaluate the performance of the machine learning model on an imbalanced dataset are:\n",
    "\n",
    "1. Confusion matrix: \n",
    "  The confusion matrix provides a way to evaluate the performance of a binary classification model by calculating the true positive, \n",
    "  true negative, false positive, and false negative rates. The confusion matrix can be used to calculate metrics such as precision,\n",
    "  recall, F1-score, and accuracy.\n",
    "\n",
    "2. Receiver operating characteristic (ROC) curve: \n",
    "  The ROC curve is a graphical representation of the performance of a binary classification model. \n",
    "  The ROC curve plots the true positive rate against the false positive rate at different classification thresholds. \n",
    "  The area under the ROC curve (AUC-ROC) is a commonly used metric to evaluate the performance of a binary classification model on imbalanced datasets.\n",
    "  \n",
    "\n",
    "3. Precision-Recall (PR) curve: \n",
    "  The PR curve is another graphical representation of the performance of a binary classification model.\n",
    "  The PR curve plots the precision against the recall at different classification thresholds. \n",
    "  The area under the PR curve (AUC-PR) is a commonly used metric to evaluate the performance of a binary classification model on imbalanced datasets.\n",
    "\n",
    "4. Stratified sampling:  \n",
    "  Stratified sampling is a technique used to ensure that the imbalanced dataset is sampled proportionally during the training and testing phases.  \n",
    "  Stratified sampling ensures that the minority class is sampled in sufficient quantities to allow the machine learning model to learn from it.\n",
    "\n",
    "5. Resampling techniques: \n",
    "    Resampling techniques such as oversampling and undersampling can be used to balance the dataset.\n",
    "    Oversampling involves duplicating samples from the minority class, while undersampling involves removing samples from the majority class.\n",
    "    However, resampling techniques should be used with caution as they can lead to overfitting and other problems.\n",
    "\n",
    "In summary, when dealing with an imbalanced dataset such as in the case of a medical diagnosis project,\n",
    "it is important to use appropriate evaluation strategies such as confusion matrix, ROC curve, PR curve, \n",
    "stratified sampling, and resampling techniques. \n",
    "These strategies can help ensure that the machine learning model's performance is accurately assessed and can help guide the choice of \n",
    "appropriate machine learning algorithms and parameters.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd425e2e-48c4-4fdc-a5a2-b380b8b9ce4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q 10 Answer:\n",
    "\"\"\"\n",
    "When dealing with an imbalanced dataset where the majority of customers report being satisfied, \n",
    "there are several methods that can be used to balance the dataset and down-sample the majority class. Some of these methods are:\n",
    "\n",
    "1. Random under-sampling: This involves randomly selecting a subset of the majority class samples to match the number of samples in the minority class.\n",
    "\n",
    "2. Cluster-based under-sampling: This involves clustering the majority class samples and selecting one sample from each cluster to match \n",
    "the number of samples in the minority class.\n",
    "\n",
    "3.Tomek links: This involves identifying pairs of samples that are the closest to each other but belong to different classes,\n",
    "and removing the majority class samples from the pair.\n",
    "\n",
    "4.Edited nearest neighbors: This involves identifying the majority class samples that are misclassified by their k-nearest neighbors\n",
    "and removing them from the dataset.\n",
    "\n",
    "Here's an example of how to perform random under-sampling on a dataset using Python:\n",
    "\n",
    "\"\"\"\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Separate majority and minority classes\n",
    "majority_class = df[df.satisfaction=='satisfied']\n",
    "minority_class = df[df.satisfaction=='unsatisfied']\n",
    "\n",
    "# Downsample majority class\n",
    "downsampled_majority = resample(majority_class, \n",
    "                                replace=False,     # Sample without replacement\n",
    "                                n_samples=len(minority_class),   # Match minority class size\n",
    "                                random_state=42)   # Reproducible results\n",
    "\n",
    "# Combine minority class and downsampled majority class\n",
    "balanced_df = pd.concat([downsampled_majority, minority_class])\n",
    "\n",
    "# Shuffle the dataset\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42)\n",
    "\"\"\"\n",
    "This code first separates the majority and minority classes, then uses the resample function from the scikit-learn library to randomly \n",
    "down-sample the majority class to match the size of the minority class. Finally, the code combines the down-sampled majority class and the \n",
    "minority class to create a balanced dataset and shuffles the data to ensure that the order of the samples is random.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95dfdb8-99ba-4e7d-8029-95f4c63e9de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q 11 Answer:\n",
    "\"\"\"\n",
    "When working with an imbalanced dataset with a low percentage of occurrences, \n",
    "there are several methods that can be used to balance the dataset and up-sample the minority class. Some of these methods are:\n",
    "\n",
    "1. Random over-sampling: This involves randomly duplicating samples from the minority class to match the number of samples in the majority class.\n",
    "\n",
    "2. Synthetic minority over-sampling technique (SMOTE): This involves generating synthetic samples from the minority class by interpolating \n",
    "between existing samples.\n",
    "\n",
    "3. Adaptive synthetic (ADASYN): This involves generating synthetic samples from the minority class based on the density of the samples \n",
    "in the feature space.\n",
    "\n",
    "4.Borderline-SMOTE: This is a variant of SMOTE that generates synthetic samples only for the minority class samples that are near the decision  \n",
    "boundary.\n",
    "\n",
    "\"\"\"\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Separate majority and minority classes\n",
    "majority_class = df[df.label==0]\n",
    "minority_class = df[df.label==1]\n",
    "\n",
    "# Upsample minority class using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_smote, y_smote = smote.fit_resample(X, y)\n",
    "\n",
    "# Combine majority class and upsampled minority class\n",
    "balanced_df = pd.concat([majority_class, pd.DataFrame(X_smote, columns=X.columns), y_smote], axis=1)\n",
    "\n",
    "# Shuffle the dataset\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42)\n",
    "\"\"\"\n",
    "This code first separates the majority and minority classes, then uses the SMOTE function from the imbalanced-learn library to up-sample the minority class by generating \n",
    "synthetic samples. Finally, the code combines the up-sampled minority class and the majority class to create a\n",
    "balanced dataset and shuffles the data to ensure that the order of the samples is random.\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
